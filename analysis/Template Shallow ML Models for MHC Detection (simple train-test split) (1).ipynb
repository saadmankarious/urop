{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f4cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple template for a ML modeling approach based on a two-way split (splitting the data into training set and a testing set).\n",
    "# This approach is convenient and a good starting point.\n",
    "# In practice, more rigorous evaluation methods like cross-validation or bootstrapping are typically employed to obtain more robust model performance and generalizability.\n",
    "\n",
    "# The script defines and evaluates several machine learning models using a two-way split approach on datasets related to bipolar disorder classification. Initially, it loads and preprocesses the training and test datasets, performing standard scaling on the features. It then iterates through a predefined set of classifiers including Random Forest, SVM, Gradient Boosting, Logistic Regression, MLP Classifier, and AdaBoost. Each model is trained on the training data and evaluated using metrics such as accuracy, precision, recall, F1-score, and confusion matrix on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70641a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing training dataset: /Users/daniel24/Documents/Research/SMHD/CymoResults/ExpTradClass/data/balancedReversedEng/bipolar_train_means_based.csv\n",
      "\n",
      "Processing test dataset: /Users/daniel24/Documents/Research/SMHD/CymoResults/ExpTradClass/data/balancedReversedEng/bipolar_test_means_based.csv\n",
      "Training and evaluating Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3s/j4wx9mgx7893ykdhlkc95jkw0000gn/T/ipykernel_63146/530964665.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating SVM...\n",
      "Training and evaluating Gradient Boosting...\n",
      "Training and evaluating Logistic Regression...\n",
      "Training and evaluating MLP Classifier...\n",
      "Training and evaluating AdaBoost...\n",
      "                 Model  Dataset  Accuracy  Precision    Recall  F1-Score  \\\n",
      "0        Random Forest  Bipolar  0.770492   0.771972  0.770492  0.770179   \n",
      "1                  SVM  Bipolar  0.795082   0.802417  0.795082  0.793832   \n",
      "2    Gradient Boosting  Bipolar  0.786885   0.786962  0.786885  0.786871   \n",
      "3  Logistic Regression  Bipolar  0.766393   0.766680  0.766393  0.766331   \n",
      "4       MLP Classifier  Bipolar  0.770492   0.770783  0.770492  0.770430   \n",
      "\n",
      "         Confusion Matrix  \n",
      "0  [[179, 65], [47, 197]]  \n",
      "1  [[175, 69], [31, 213]]  \n",
      "2  [[194, 50], [54, 190]]  \n",
      "3  [[183, 61], [53, 191]]  \n",
      "4  [[184, 60], [52, 192]]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "############ Creating a train / test split #######################\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming your dataset is in a CSV file, adjust as needed\n",
    "df = pd.read_csv('~/code/urop/reddit/data/2019_output/ann-combined.csv')\n",
    "\n",
    "# Define your features and target variable\n",
    "X = df.drop(columns=['MHC'])  # Replace 'MHC' with the name of your class label column\n",
    "y = df['MHC']  # Replace 'MHC' with the name of your class label column\n",
    "\n",
    "# Perform the train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Combine X and y back into train and test DataFrames\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Optionally save the splits to CSV files\n",
    "train_df.to_csv('train_dataset.csv', index=False)\n",
    "test_df.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "\n",
    "################  Setting up the models  ###################\n",
    "\n",
    "\n",
    "# Define models dictionary globally\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42, probability=True),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"MLP Classifier\": MLPClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Function to load and preprocess dataset\n",
    "def load_and_preprocess_dataset(dataset_path):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={'ID': 'userID', 'MHC': 'condition'}, inplace=True)\n",
    "    \n",
    "    # Assuming 'condition' is the target column\n",
    "    X = df.drop(columns=['condition', 'userID'])  # Adjust columns to drop as needed\n",
    "    y = df['condition']\n",
    "    \n",
    "    # Standard scaling of features if necessary\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, X, y\n",
    "\n",
    "# Function to train and evaluate models using dedicated training and test datasets\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, results_df, dataset_name):\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training and evaluating {name}...\")\n",
    "        \n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "        confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Append results to DataFrame\n",
    "        temp_df = pd.DataFrame({\n",
    "            'Model': [name],\n",
    "            'Dataset': [dataset_name],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Precision': [precision],\n",
    "            'Recall': [recall],\n",
    "            'F1-Score': [fscore],\n",
    "            'Confusion Matrix': [confusion_matrix_result]\n",
    "        })\n",
    "        results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Initialize DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Dataset', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Confusion Matrix'])\n",
    "\n",
    "# Specify paths to training and test datasets\n",
    "train_dataset_path = \"/Users/daniel24/Documents/Research/SMHD/CymoResults/ExpTradClass/data/balancedReversedEng/bipolar_train_means_based.csv\"\n",
    "test_dataset_path = \"/Users/daniel24/Documents/Research/SMHD/CymoResults/ExpTradClass/data/balancedReversedEng/bipolar_test_means_based.csv\"\n",
    "\n",
    "print(f\"\\nProcessing training dataset: {train_dataset_path}\")\n",
    "X_train_scaled, X_train_raw, y_train = load_and_preprocess_dataset(train_dataset_path)\n",
    "\n",
    "print(f\"\\nProcessing test dataset: {test_dataset_path}\")\n",
    "X_test_scaled, X_test_raw, y_test = load_and_preprocess_dataset(test_dataset_path)\n",
    "\n",
    "# Train and evaluate models\n",
    "results_df = train_and_evaluate_models(X_train_scaled, y_train, X_test_scaled, y_test, results_df, \"Bipolar\")\n",
    "\n",
    "# Write results to a CSV file\n",
    "results_df.to_csv(\"/Users/daniel24/Documents/Research/SMHD/CymoResults/ExpTradClass/data/balancedReversedEng/results/bipolar_model_evaluation_results.csv\", index=False)\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcfff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
